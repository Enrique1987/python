{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f65fef8",
   "metadata": {},
   "source": [
    "## Summarizing speeches with NLP: I have a dream speech\n",
    "\n",
    "Acording with International Data Corporation by 2025 we´ll be generatin 175 trillons gigabytes of digital data pro year. But most of this data-up to 95% will be unstructure, wich means it´s not organized into useful databases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759f8b1e",
   "metadata": {},
   "source": [
    "- In this notebook we will learn to use the NLTK to geneare a summary of one of the most famouse speeches of all time \"I have a dream\".\n",
    "- After learn the basis we will use gesim to summarize speachs.\n",
    "- Finnaly we will use a word cloud to produce a fun visual summary of the most fequently used words in a novel.\n",
    "\n",
    "In data mining and ml there are two approaches to summarizing text: extraction and abstraction.\n",
    "\n",
    "`Extraction-based:`  Uses various weighting functions to rank sentences by perceveived importance. Words uses more often are considered more important. Consequently, sentences containing those words are considred more important. The overall behavior is like using a yellow highligther to manually select keywords and sentences without altering the text.\n",
    "\n",
    "This technique is good at pulling out important words and phrases.\n",
    "\n",
    "`Abstraction:` Deeper compehension of the document to capture intent and produce more humal-like paraphasing. This includes creating completly new sentences.\n",
    "\n",
    "Abstraction algorithms requred advanced and complicated deep learning methods and sophisticated language modeling.\n",
    "\n",
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4e666d",
   "metadata": {},
   "source": [
    "   1. Point  web page containing the “I Have a Dream” speech.\n",
    "   2. Loads the text as a string\n",
    "   3. Tokenizes the text into words and sentences\n",
    "   4. Removes stop words with no contextual content\n",
    "   5. Counts the remaining words\n",
    "   6. Uses the counts to rank the sentences \n",
    "   7. Displays the highest-ranking sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10536d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import bs4\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c676bc4",
   "metadata": {},
   "source": [
    "**word_tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99531cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', '-', 'Hi', 'this', 'is', 'a', 'test', 'about', 'tokenize', 'from', 'nltk', 'that', 'is', 'just', 'a', 'test', '.']\n",
      "['test', '-', 'Hi', 'this', 'is', 'a', 'test', 'about', 'tokenize', 'from', 'nltk', 'that', 'is', 'just', 'a', 'test.']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'test - Hi this is a test about tokenize from nltk that is just a test.'\n",
    "test_tokenize = nltk.word_tokenize(sentence)\n",
    "print(test_tokenize)\n",
    "print(list(sentence.split(' ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecc460e",
   "metadata": {},
   "source": [
    "**Tokenize vs list(split(' '))**\n",
    "- `Tokenize` performe better if there are errors in the sentences as 2 white spaces.(take care about comma, point..)\n",
    "- Use `List` had the advantage that its alredy in python so we do no need to import anything (probably the cluster we are using doesn´t have that library install)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ed2d66",
   "metadata": {},
   "source": [
    "**sentence_tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "add63611",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_test = ''' Mr. Sherlock Holmes, who was usually very late in the mornings, save upon those not infrequent occasions when he was up all night, was seated at the breakfast table. I stood upon the hearth-rug and picked up the stick which our visitor had left behind him the night before. It was a fine, thick piece of wood, bulbous-headed, of the sort which is known as a “Penang lawyer.” Just under the head was a broad silver band nearly an inch across. “To James Mortimer, M.R.C.S., from his friends of the C.C.H.,” was engraved upon it, with the date “1884.” It was just such a stick as the old-fashioned family practitioner used to carry--dignified, solid, and reassuring.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb85e2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mr. Sherlock Holmes, who was usually very late in the mornings, save upon those not infrequent occasions when he was up all night, was seated at the breakfast table.\n",
      "I stood upon the hearth-rug and picked up the stick which our visitor had left behind him the night before.\n",
      "It was a fine, thick piece of wood, bulbous-headed, of the sort which is known as a “Penang lawyer.” Just under the head was a broad silver band nearly an inch across.\n",
      "“To James Mortimer, M.R.C.S., from his friends of the C.C.H.,” was engraved upon it, with the date “1884.” It was just such a stick as the old-fashioned family practitioner used to carry--dignified, solid, and reassuring.\n"
     ]
    }
   ],
   "source": [
    "sentences_test = nltk.sent_tokenize(speech_test)\n",
    "for sentence in sentences_test:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db6faa9",
   "metadata": {},
   "source": [
    "**Frequency Diccionary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00d04a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "frek_disc = nltk.FreqDist(nltk.word_tokenize(sentence.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "621f9024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 1\n",
      "hi 1\n",
      "this 1\n",
      "about 1\n",
      "tokenize 1\n",
      "from 1\n",
      "nltk 1\n",
      "that 1\n",
      "just 1\n",
      ". 1\n",
      "is 2\n",
      "a 2\n",
      "test 3\n"
     ]
    }
   ],
   "source": [
    "frek_disc_ord = {k: v for k, v in sorted(frek_disc.items(), key=lambda item: item[1])}\n",
    "for key, value in frek_disc_ord.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbfff892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(speech_edit):\n",
    "    \"\"\"Remove stop words from string and return string.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    speech_edit_no_stop = ''\n",
    "    for word in nltk.word_tokenize(speech_edit):\n",
    "        if word.lower() not in stop_words:\n",
    "            speech_edit_no_stop += word + ' '  \n",
    "    return speech_edit_no_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5469a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_freq(speech_edit_no_stop):\n",
    "    \"\"\"Return a dictionary of word frequency in a string.\"\"\"\n",
    "    word_freq = nltk.FreqDist(nltk.word_tokenize(speech_edit_no_stop.lower()))\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96f7ef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentences(speech, word_freq, max_words):\n",
    "    \"\"\"Return dictionary of sentence scores based on word frequency.\"\"\"\n",
    "    sent_scores = dict()\n",
    "    sentences = nltk.sent_tokenize(speech)\n",
    "    for sent in sentences:\n",
    "        sent_scores[sent] = 0\n",
    "        words = nltk.word_tokenize(sent.lower())\n",
    "        sent_word_count = len(words)\n",
    "        if sent_word_count <= int(max_words):\n",
    "            for word in words:\n",
    "                if word in word_freq.keys():\n",
    "                    sent_scores[sent] += word_freq[word]\n",
    "            sent_scores[sent] = sent_scores[sent] / sent_word_count\n",
    "    return sent_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f140354",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.analytictech.com/mb021/mlk.htm'\n",
    "page = requests.get(url)\n",
    "page.raise_for_status()\n",
    "soup = bs4.BeautifulSoup(page.text, 'html.parser')\n",
    "p_elems = [element.text for element in soup.find_all('p')]\n",
    "speech = ' '.join(p_elems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097abdba",
   "metadata": {},
   "source": [
    "**Use of \\s+**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bc8758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = 'hola juanito    me gustaria invitarte a tomar una cerveza   cuando podrias venir?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8f65811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola juanito me gustaria invitarte a tomar una cerveza cuando podrias venir?\n"
     ]
    }
   ],
   "source": [
    "speech_2 = re.sub('\\s+', ' ', test_text) \n",
    "print(speech_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5369e6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter max words per sentence for summary: 3\n",
      "Enter number of sentences for summary: 5\n"
     ]
    }
   ],
   "source": [
    "# Request input.\n",
    "while True:\n",
    "    max_words = input(\"Enter max words per sentence for summary: \")\n",
    "    num_sents = input(\"Enter number of sentences for summary: \")\n",
    "    if max_words.isdigit() and num_sents.isdigit():\n",
    "        break\n",
    "    else:\n",
    "        print(\"\\nInput must be in whole numbers.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d359f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_edit_no_stop = remove_stop_words(speech_edit)\n",
    "word_freq = get_word_freq(speech_edit_no_stop)\n",
    "sent_scores = score_sentences(speech, word_freq, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e88560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top-ranked sentences.\n",
    "counts = Counter(sent_scores)\n",
    "summary = counts.most_common(int(num_sents))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3dc457",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSUMMARY:\")\n",
    "    for i in summary:\n",
    "        print(i[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
